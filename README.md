## Projects Completed

Here are 6 projects that I have completed over the past year.

**Project 1:** I developed a multi-threaded HTTP server using C, which involved creating multiple threads, implementing a bounded buffer, and utilizing mutex locks to ensure an atomic and coherent process.

**Project 2:** I built an NFL Chatbot powered by OpenAIâ€™s Large Language Models, enabling accurate responses to user queries related to the NFL. I stored over 1000 lines of raw NFL data as word embeddings in a Pinecone Vector Database, leveraging semantic search to identify and retrieve the most relevant information.

**Project 3:** I effectively employed Q-learning, depth-first search, breadth-first search, and A* search algorithms in Python to enhance Pac-Man's gameplay. This optimization was done to improve both its decision-making process and pathfinding for maximum food consumption.

**Project 4:** I built a Slack app using React for the front end, PostgreSQL for the database, and Node.js for the backend. I inserted user data into the database using SQL and retrieved each unique user workspace, channel, and message through server calls from the backend. The front end was then used to display all this information seamlessly, providing a comprehensive and user-friendly experience.

**Project 5:** In this project, I developed unigram, bigram, and trigram language models to predict word sequences. These models were trained to calculate the probabilities of words based on their preceding context. To evaluate the accuracy of these models, I used metrics like perplexity, which measures how well the models predict the next word in a sequence. This approach allowed me to assess the performance of each model in capturing the patterns and structures in the language data.

**Project 6:** I built an MCP (Modular Code Processing) server using FastAPI, OpenAI's GPT-4, and the Wikipedia API to create an intelligent question-answering system. The server modularly handles API endpoints for Wikipedia search, summary retrieval, and chatbot interaction, allowing the LLM to dynamically extract relevant information from external tools. This project demonstrated how to orchestrate tool use via an LLM and serve modular APIs for intelligent, context-aware responses.

